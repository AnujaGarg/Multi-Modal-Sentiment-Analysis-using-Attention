{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0018bd60",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d55cd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ./sorted_data\n",
      " - loading books positive: 1000 texts\n",
      " - loading books negative: 1000 texts\n",
      " - loading dvd positive: 1000 texts\n",
      " - loading dvd negative: 1000 texts\n",
      " - loading electronics positive: 1000 texts\n",
      " - loading electronics negative: 1000 texts\n",
      " - loading kitchen positive: 1000 texts\n",
      " - loading kitchen negative: 1000 texts\n",
      "data loaded\n",
      " - texts: 8000\n",
      " - s_labels: 8000\n",
      " - d_labels: 8000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "MDSD_PATH = os.path.expanduser('./')\n",
    "\n",
    "DOMAINS = ('books', 'dvd', 'electronics', 'kitchen')\n",
    "\n",
    "\n",
    "def load_mdsd(domains, n_labeled=None):\n",
    "    sorted_data_path = os.path.join(MDSD_PATH, 'sorted_data')\n",
    "    print('loading data from {}'.format(sorted_data_path))\n",
    "    texts = []\n",
    "    s_labels = []\n",
    "    d_labels = []\n",
    "    sentiments = ('positive', 'negative')\n",
    "    for d_id, d_name in enumerate(domains):\n",
    "        for s_id, s_name in zip((1, 0, -1), sentiments):\n",
    "            fpath = os.path.join(sorted_data_path, d_name, s_name + '.review')\n",
    "            print(' - loading', d_name, s_name, end='')\n",
    "            count = 0\n",
    "            text = ''\n",
    "            in_review_text = False\n",
    "            with open(fpath, encoding='utf8', errors='ignore') as fr:\n",
    "                for line in fr:\n",
    "                    if '<review_text>' in line:\n",
    "                        text = ''\n",
    "                        in_review_text = True\n",
    "                        continue\n",
    "                    if '</review_text>' in line:\n",
    "                        in_review_text = False\n",
    "                        text = text.lower().replace('\\n', ' ').strip()\n",
    "                        text = re.sub(r'&[a-z]+;', '', text)\n",
    "                        text = re.sub(r'\\s+', ' ', text)\n",
    "                        texts.append(text)\n",
    "                        s_labels.append(s_id)\n",
    "                        d_labels.append(d_id)\n",
    "                        count += 1\n",
    "                    if in_review_text:\n",
    "                        text += line\n",
    "                    if (s_id >= 0) and n_labeled and (count == n_labeled):\n",
    "                        break\n",
    "            print(': %d texts' % count)\n",
    "    print('data loaded')\n",
    "    s_labels = np.asarray(s_labels, dtype='int')\n",
    "    d_labels = np.asarray(d_labels, dtype='int')\n",
    "    print(' - texts:', len(texts))\n",
    "    print(' - s_labels:', len(s_labels))\n",
    "    print(' - d_labels:', len(d_labels))\n",
    "\n",
    "    return texts, s_labels, d_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd4b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from keras import layers\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "glove_path = os.path.expanduser('./glove/')\n",
    "assert os.path.exists(glove_path)\n",
    "\n",
    "\n",
    "def load_glove(path=glove_path, embedding_dim=300, corpus_size=6, desired=None, verbose=False):\n",
    "    if embedding_dim != 300:\n",
    "        assert embedding_dim in (50, 100, 200),\n",
    "        fpath = os.path.join(path, 'glove.6B.{}d.txt'.format(embedding_dim))\n",
    "    else:\n",
    "        assert corpus_size in (6, 42, 840), \n",
    "        fpath = os.path.join(path, 'glove.{}B.300d.txt'.format(corpus_size))\n",
    "    word2vec = {}\n",
    "    print('loading glove from', fpath)\n",
    "    f = open(fpath, 'r', encoding='utf8', errors='ignore')\n",
    "    for line in tqdm(f, desc='glove') if verbose else f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # the word\n",
    "        if not desired or word in desired:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            word2vec[word] = coefs\n",
    "    f.close()\n",
    "    print('glove info: {} words, {} dims'.format(len(word2vec), embedding_dim))\n",
    "    return word2vec\n",
    "\n",
    "\n",
    "def get_embedding_mat(embeddings, word2index, embedding_dim, random_uniform_level=0.01, idx_from=2):\n",
    "    # embedding_mat = np.zeros((n_words, embedding_dim))\n",
    "    n_words = len(word2index)\n",
    "    for idx in range(0, idx_from):\n",
    "        if idx in word2index.values():\n",
    "            n_words -= 1\n",
    "    n_words += idx_from\n",
    "    embedding_mat = np.random.uniform(low=-random_uniform_level, high=random_uniform_level, size=(n_words, embedding_dim))\n",
    "    embedding_mat[0] = np.zeros(embedding_dim)\n",
    "    for word, idx in word2index.items():\n",
    "        if idx < idx_from:\n",
    "            continue\n",
    "        embedding_vec = embeddings.get(word)\n",
    "        if embedding_vec is not None:  \n",
    "            embedding_mat[idx] = embedding_vec\n",
    "    return embedding_mat\n",
    "\n",
    "\n",
    "def att_process(candidates, att, activation='tanh'):\n",
    "    att_dim = K.int_shape(att)[-1]\n",
    "    candidates2 = layers.TimeDistributed(\n",
    "        layers.Dense(att_dim, activation=activation))(candidates)\n",
    "    dotted = layers.dot([candidates2, att], axes=(2, 1), normalize=True)\n",
    "    weights = layers.Activation('softmax')(dotted)  # (*, maxlen), sums up to 1\n",
    "    weighted = layers.dot([candidates, weights], axes=(1, 1))\n",
    "    return weighted, weights\n",
    "\n",
    "\n",
    "class UpdateMonitor(Callback):\n",
    "    def __init__(self):\n",
    "        super(UpdateMonitor, self).__init__()\n",
    "        self.weights = None\n",
    "\n",
    "    @classmethod\n",
    "    def _get_updates(cls, old_weights, new_weights):\n",
    "        if not old_weights:\n",
    "            old_weights = new_weights\n",
    "        updates = []\n",
    "        for old_layerwise_weights, new_layerwise_weights in zip(old_weights, new_weights):\n",
    "            if len(old_layerwise_weights) == 0 or len(new_layerwise_weights) == 0:\n",
    "                updates.append(None)\n",
    "            else:\n",
    "                w1, w2 = old_layerwise_weights[0], new_layerwise_weights[0]  # only check the first weight of a layer\n",
    "                updates.append(norm(w2 - w1) / norm(w2))\n",
    "        return updates\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        new_weights = _get_weights(self.model)\n",
    "        updates = self._get_updates(old_weights=self.weights, new_weights=new_weights)\n",
    "        self.weights = new_weights  # update\n",
    "        updates_info = ', '.join('{:.4f}'.format(1e3 * update) if update else '-' for update in updates)\n",
    "        print('- updates: 1e-3 * [{}]'.format(updates_info))\n",
    "\n",
    "\n",
    "def _get_weights(model):\n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        # if no weights, return value is []\n",
    "        weights.append(layer.get_weights())\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814fa89c",
   "metadata": {},
   "source": [
    "### Training & Validating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412efb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data: Multi-Domain Sentiment Dataset v2\n",
      "loading data from ./sorted_data\n",
      " - loading books positive: 1000 texts\n",
      " - loading books negative: 1000 texts\n",
      " - loading dvd positive: 1000 texts\n",
      " - loading dvd negative: 1000 texts\n",
      " - loading electronics positive: 1000 texts\n",
      " - loading electronics negative: 1000 texts\n",
      " - loading kitchen positive: 1000 texts\n",
      " - loading kitchen negative: 1000 texts\n",
      "data loaded\n",
      " - texts: 8000\n",
      " - s_labels: 8000\n",
      " - d_labels: 8000\n",
      "building vocabulary\n",
      "maxlen: 461\n",
      "n_words: 45077\n",
      "data encoding\n",
      "labeled data: domain & train/val/test splitting\n",
      "books splitting\n",
      " * all: (2000, 461) (2000,)\n",
      " * X: (1400, 461) (398, 461) (202, 461)\n",
      " * y: (1400,) (398,) (202,)\n",
      "dvd splitting\n",
      " * all: (2000, 461) (2000,)\n",
      " * X: (1400, 461) (398, 461) (202, 461)\n",
      " * y: (1400,) (398,) (202,)\n",
      "electronics splitting\n",
      " * all: (2000, 461) (2000,)\n",
      " * X: (1400, 461) (398, 461) (202, 461)\n",
      " * y: (1400,) (398,) (202,)\n",
      "kitchen splitting\n",
      " * all: (2000, 461) (2000,)\n",
      " * X: (1400, 461) (398, 461) (202, 461)\n",
      " * y: (1400,) (398,) (202,)\n",
      "combined labeled data:\n",
      "  - train: (5600, 461) (5600, 2) (5600, 4)\n",
      "  - val: (1592, 461) (1592, 2) (1592, 4)\n",
      "  - test: (808, 461) (808, 2) (808, 4)\n",
      "  - test for boo: (202, 461) (202, 2) (202, 4)\n",
      "  - test for dvd: (202, 461) (202, 2) (202, 4)\n",
      "  - test for ele: (202, 461) (202, 2) (202, 4)\n",
      "  - test for kit: (202, 461) (202, 2) (202, 4)\n",
      "loading word embeddings from glove\n",
      "loading glove from ./glove/glove.6B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-28993d6ebf1a>:68: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  _seqs = _seqs[indices]\n",
      "<ipython-input-3-28993d6ebf1a>:69: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  _slabels = _slabels[indices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove info: 35122 words, 300 dims\n",
      "processing embedding matrix\n",
      "\n",
      "building the model\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 461)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 461, 300)     13523100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 461, 300)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 600)          1442400     spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 461, 600)     1442400     spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          60100       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 461, 100)     60100       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 100)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 461)          0           time_distributed[0][0]           \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 461)          0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 600)          0           bidirectional_1[0][0]            \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          60100       dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s_pred (Dense)                  (None, 2)            202         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d_pred (Dense)                  (None, 4)            404         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,588,806\n",
      "Trainable params: 16,588,806\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "training model\n",
      "Epoch 1/20\n",
      "88/88 - 459s - loss: 0.7488 - s_pred_loss: 0.6934 - d_pred_loss: 1.3854 - s_pred_acc: 0.5023 - d_pred_acc: 0.2539 - val_loss: 0.7485 - val_s_pred_loss: 0.6932 - val_d_pred_loss: 1.3833 - val_s_pred_acc: 0.4893 - val_d_pred_acc: 0.2601\n",
      "- updates: 1e-3 * [-, -, -, -, -, -, -, -, -, -, -, -, -, -, -]\n",
      "Epoch 2/20\n",
      "88/88 - 474s - loss: 0.7489 - s_pred_loss: 0.6935 - d_pred_loss: 1.3839 - s_pred_acc: 0.4843 - d_pred_acc: 0.2586 - val_loss: 0.7485 - val_s_pred_loss: 0.6932 - val_d_pred_loss: 1.3832 - val_s_pred_acc: 0.4899 - val_d_pred_acc: 0.2594\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0060, 0.0420, 0.0258, 0.0022, -, -, -, -, 0.0671, -, 0.0587, 0.0415]\n",
      "Epoch 3/20\n",
      "88/88 - 485s - loss: 0.7492 - s_pred_loss: 0.6938 - d_pred_loss: 1.3839 - s_pred_acc: 0.4911 - d_pred_acc: 0.2562 - val_loss: 0.7485 - val_s_pred_loss: 0.6931 - val_d_pred_loss: 1.3830 - val_s_pred_acc: 0.4918 - val_d_pred_acc: 0.2607\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0060, 0.0399, 0.0262, 0.0018, -, -, -, -, 0.0695, -, 0.0631, 0.0415]\n",
      "Epoch 4/20\n",
      "88/88 - 9162s - loss: 0.7484 - s_pred_loss: 0.6930 - d_pred_loss: 1.3840 - s_pred_acc: 0.4970 - d_pred_acc: 0.2605 - val_loss: 0.7484 - val_s_pred_loss: 0.6931 - val_d_pred_loss: 1.3829 - val_s_pred_acc: 0.4937 - val_d_pred_acc: 0.2601\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0061, 0.0436, 0.0254, 0.0018, -, -, -, -, 0.0714, -, 0.0644, 0.0410]\n",
      "Epoch 5/20\n",
      "88/88 - 443s - loss: 0.7488 - s_pred_loss: 0.6934 - d_pred_loss: 1.3855 - s_pred_acc: 0.4977 - d_pred_acc: 0.2577 - val_loss: 0.7483 - val_s_pred_loss: 0.6930 - val_d_pred_loss: 1.3827 - val_s_pred_acc: 0.4969 - val_d_pred_acc: 0.2588\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0059, 0.0468, 0.0263, 0.0022, -, -, -, -, 0.0796, -, 0.0663, 0.0406]\n",
      "Epoch 6/20\n",
      "88/88 - 450s - loss: 0.7492 - s_pred_loss: 0.6939 - d_pred_loss: 1.3846 - s_pred_acc: 0.4827 - d_pred_acc: 0.2546 - val_loss: 0.7483 - val_s_pred_loss: 0.6930 - val_d_pred_loss: 1.3826 - val_s_pred_acc: 0.4987 - val_d_pred_acc: 0.2582\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0061, 0.0405, 0.0261, 0.0029, -, -, -, -, 0.0665, -, 0.0584, 0.0419]\n",
      "Epoch 7/20\n",
      "88/88 - 459s - loss: 0.7489 - s_pred_loss: 0.6936 - d_pred_loss: 1.3826 - s_pred_acc: 0.4921 - d_pred_acc: 0.2689 - val_loss: 0.7483 - val_s_pred_loss: 0.6930 - val_d_pred_loss: 1.3825 - val_s_pred_acc: 0.5013 - val_d_pred_acc: 0.2588\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0059, 0.0433, 0.0253, 0.0019, -, -, -, -, 0.0707, -, 0.0631, 0.0407]\n",
      "Epoch 8/20\n",
      "88/88 - 1166s - loss: 0.7487 - s_pred_loss: 0.6934 - d_pred_loss: 1.3832 - s_pred_acc: 0.4941 - d_pred_acc: 0.2671 - val_loss: 0.7482 - val_s_pred_loss: 0.6929 - val_d_pred_loss: 1.3823 - val_s_pred_acc: 0.5031 - val_d_pred_acc: 0.2601\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0058, 0.0441, 0.0257, 0.0019, -, -, -, -, 0.0765, -, 0.0724, 0.0402]\n",
      "Epoch 9/20\n",
      "88/88 - 521s - loss: 0.7490 - s_pred_loss: 0.6937 - d_pred_loss: 1.3827 - s_pred_acc: 0.4814 - d_pred_acc: 0.2634 - val_loss: 0.7482 - val_s_pred_loss: 0.6929 - val_d_pred_loss: 1.3822 - val_s_pred_acc: 0.5044 - val_d_pred_acc: 0.2594\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0058, 0.0426, 0.0253, 0.0028, -, -, -, -, 0.0770, -, 0.0679, 0.0389]\n",
      "Epoch 10/20\n",
      "88/88 - 502s - loss: 0.7488 - s_pred_loss: 0.6935 - d_pred_loss: 1.3824 - s_pred_acc: 0.4982 - d_pred_acc: 0.2618 - val_loss: 0.7482 - val_s_pred_loss: 0.6929 - val_d_pred_loss: 1.3820 - val_s_pred_acc: 0.5057 - val_d_pred_acc: 0.2607\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0058, 0.0400, 0.0263, 0.0024, -, -, -, -, 0.0685, -, 0.0616, 0.0399]\n",
      "Epoch 11/20\n",
      "88/88 - 551s - loss: 0.7484 - s_pred_loss: 0.6931 - d_pred_loss: 1.3831 - s_pred_acc: 0.4966 - d_pred_acc: 0.2605 - val_loss: 0.7480 - val_s_pred_loss: 0.6928 - val_d_pred_loss: 1.3819 - val_s_pred_acc: 0.5025 - val_d_pred_acc: 0.2626\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0056, 0.0422, 0.0246, 0.0026, -, -, -, -, 0.0693, -, 0.0644, 0.0400]\n",
      "Epoch 12/20\n",
      "88/88 - 639s - loss: 0.7482 - s_pred_loss: 0.6928 - d_pred_loss: 1.3839 - s_pred_acc: 0.5011 - d_pred_acc: 0.2587 - val_loss: 0.7480 - val_s_pred_loss: 0.6927 - val_d_pred_loss: 1.3818 - val_s_pred_acc: 0.5025 - val_d_pred_acc: 0.2651\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0058, 0.0480, 0.0247, 0.0025, -, -, -, -, 0.0855, -, 0.0810, 0.0399]\n",
      "Epoch 13/20\n",
      "88/88 - 655s - loss: 0.7487 - s_pred_loss: 0.6934 - d_pred_loss: 1.3837 - s_pred_acc: 0.5000 - d_pred_acc: 0.2657 - val_loss: 0.7479 - val_s_pred_loss: 0.6926 - val_d_pred_loss: 1.3816 - val_s_pred_acc: 0.5038 - val_d_pred_acc: 0.2632\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0058, 0.0444, 0.0250, 0.0022, -, -, -, -, 0.0673, -, 0.0607, 0.0397]\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 628s - loss: 0.7483 - s_pred_loss: 0.6930 - d_pred_loss: 1.3835 - s_pred_acc: 0.5013 - d_pred_acc: 0.2670 - val_loss: 0.7478 - val_s_pred_loss: 0.6926 - val_d_pred_loss: 1.3815 - val_s_pred_acc: 0.5031 - val_d_pred_acc: 0.2644\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0056, 0.0457, 0.0248, 0.0026, -, -, -, -, 0.0713, -, 0.0596, 0.0400]\n",
      "Epoch 15/20\n",
      "88/88 - 582s - loss: 0.7489 - s_pred_loss: 0.6936 - d_pred_loss: 1.3827 - s_pred_acc: 0.4946 - d_pred_acc: 0.2655 - val_loss: 0.7478 - val_s_pred_loss: 0.6925 - val_d_pred_loss: 1.3813 - val_s_pred_acc: 0.5057 - val_d_pred_acc: 0.2644\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0059, 0.0472, 0.0246, 0.0025, -, -, -, -, 0.0730, -, 0.0687, 0.0393]\n",
      "Epoch 16/20\n",
      "88/88 - 606s - loss: 0.7477 - s_pred_loss: 0.6924 - d_pred_loss: 1.3831 - s_pred_acc: 0.5030 - d_pred_acc: 0.2620 - val_loss: 0.7477 - val_s_pred_loss: 0.6924 - val_d_pred_loss: 1.3812 - val_s_pred_acc: 0.5044 - val_d_pred_acc: 0.2619\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0061, 0.0459, 0.0246, 0.0027, -, -, -, -, 0.0707, -, 0.0572, 0.0406]\n",
      "Epoch 17/20\n",
      "88/88 - 611s - loss: 0.7482 - s_pred_loss: 0.6930 - d_pred_loss: 1.3813 - s_pred_acc: 0.5063 - d_pred_acc: 0.2668 - val_loss: 0.7476 - val_s_pred_loss: 0.6924 - val_d_pred_loss: 1.3811 - val_s_pred_acc: 0.5044 - val_d_pred_acc: 0.2626\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0057, 0.0496, 0.0247, 0.0029, -, -, -, -, 0.0778, -, 0.0692, 0.0396]\n",
      "Epoch 18/20\n",
      "88/88 - 610s - loss: 0.7479 - s_pred_loss: 0.6925 - d_pred_loss: 1.3840 - s_pred_acc: 0.5123 - d_pred_acc: 0.2648 - val_loss: 0.7475 - val_s_pred_loss: 0.6923 - val_d_pred_loss: 1.3810 - val_s_pred_acc: 0.5057 - val_d_pred_acc: 0.2663\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0059, 0.0506, 0.0251, 0.0037, -, -, -, -, 0.0812, -, 0.0666, 0.0379]\n",
      "Epoch 19/20\n",
      "88/88 - 3200s - loss: 0.7480 - s_pred_loss: 0.6926 - d_pred_loss: 1.3844 - s_pred_acc: 0.5013 - d_pred_acc: 0.2654 - val_loss: 0.7475 - val_s_pred_loss: 0.6922 - val_d_pred_loss: 1.3808 - val_s_pred_acc: 0.5063 - val_d_pred_acc: 0.2670\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0056, 0.0531, 0.0253, 0.0029, -, -, -, -, 0.0922, -, 0.0794, 0.0377]\n",
      "Epoch 20/20\n",
      "88/88 - 492s - loss: 0.7479 - s_pred_loss: 0.6926 - d_pred_loss: 1.3829 - s_pred_acc: 0.5068 - d_pred_acc: 0.2704 - val_loss: 0.7474 - val_s_pred_loss: 0.6922 - val_d_pred_loss: 1.3807 - val_s_pred_acc: 0.5069 - val_d_pred_acc: 0.2657\n",
      "- updates: 1e-3 * [-, 0.0001, -, 0.0059, 0.0503, 0.0247, 0.0031, -, -, -, -, 0.0773, -, 0.0671, 0.0402]\n",
      "\n",
      "Test evaluation:\n",
      "boo acc: 0.5099\n",
      "dvd acc: 0.5099\n",
      "ele acc: 0.4950\n",
      "kit acc: 0.5000\n",
      "\n",
      "process finished ~~~\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras import layers, callbacks\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(2333)\n",
    "tf.random.set_seed(2333)\n",
    "\n",
    "\n",
    "\n",
    "# domains\n",
    "DOMAINS = ('books', 'dvd', 'electronics', 'kitchen')\n",
    "\n",
    "\n",
    "class SharedData:\n",
    "    \"\"\"global data\"\"\"\n",
    "    coef = 0.04  \n",
    "\n",
    "    # model params\n",
    "    embed_dim = 300  \n",
    "    rnn_dim = 300  \n",
    "    hidden_dim = 100  \n",
    "    embed_dropout = 0.2 \n",
    "    fc_dropout = 0.2  \n",
    "    batch_size = 64  \n",
    "    epochs = 20  #epochs\n",
    "    activation = 'relu'  \n",
    "    optimizer = 'adadelta'  \n",
    "    RNN = layers.LSTM  \n",
    "\n",
    "    \n",
    "    lr_factor = 0.1  \n",
    "    lr_patience = 1  \n",
    "    stop_patience = 2  \n",
    "\n",
    "    \n",
    "    glove_corpus = 6 \n",
    "    min_count = 1  \n",
    "    max_words = None  \n",
    "    n_words = None\n",
    "    maxlen = None\n",
    "    word2index = None\n",
    "    wv_weights = None  \n",
    "\n",
    "\n",
    "SD = SharedData()\n",
    "\n",
    "\n",
    "def _tvt_split(_seqs, _slabels, splits=(7, 2, 1)):\n",
    "    \"\"\"train/val/test split for one single domain\"\"\"\n",
    "    assert len(_seqs) == len(_slabels)\n",
    "    splits = np.asarray(splits)\n",
    "    splits = np.cumsum(splits / splits.sum())\n",
    "    indices = [range(len(_seqs))]\n",
    "    np.random.shuffle(indices)\n",
    "    _seqs = _seqs[indices]\n",
    "    _slabels = _slabels[indices]\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = [], [], [], [], [], []\n",
    "    for slabel in sorted(np.unique(_slabels)):\n",
    "        seqs_ofs = _seqs[_slabels == slabel]\n",
    "        slabels_ofs = _slabels[_slabels == slabel]\n",
    "        split_ats = np.asarray(splits * len(seqs_ofs), dtype=int)\n",
    "        X_train.extend(seqs_ofs[:split_ats[0]])\n",
    "        X_val.extend(seqs_ofs[split_ats[0]:split_ats[1]])\n",
    "        X_test.extend(seqs_ofs[split_ats[1]:])\n",
    "        y_train.extend(slabels_ofs[:split_ats[0]])\n",
    "        y_val.extend(slabels_ofs[split_ats[0]:split_ats[1]])\n",
    "        y_test.extend(slabels_ofs[split_ats[1]:])\n",
    "    X_train = np.asarray(X_train, dtype='int')\n",
    "    X_val = np.asarray(X_val, dtype='int')\n",
    "    X_test = np.asarray(X_test, dtype='int')\n",
    "    y_train = np.asarray(y_train, dtype='int')\n",
    "    y_val = np.asarray(y_val, dtype='int')\n",
    "    y_test = np.asarray(y_test, dtype='int')\n",
    "    print(' * X:', X_train.shape, X_val.shape, X_test.shape)\n",
    "    print(' * y:', y_train.shape, y_val.shape, y_test.shape)\n",
    "    return (X_train, X_val, X_test), (y_train, y_val, y_test)\n",
    "\n",
    "\n",
    "def make_data():\n",
    "    global SD\n",
    "\n",
    "    print('loading data: Multi-Domain Sentiment Dataset v2')\n",
    "    texts, s_labels, d_labels = load_mdsd(domains=DOMAINS)\n",
    "\n",
    "    # build vocabulary \n",
    "    print('building vocabulary')\n",
    "    texts_tokens = []\n",
    "    lens = []\n",
    "    for text in texts:\n",
    "        words = word_tokenize(text)\n",
    "        for idx, word in enumerate(words):\n",
    "            if word.isdigit():\n",
    "                words[idx] = '<NUM>'  \n",
    "        texts_tokens.append(words)\n",
    "        lens.append(len(words))\n",
    "    maxlen = int(np.percentile(lens, 95))\n",
    "    print('maxlen:', maxlen)\n",
    "    counter = Counter()\n",
    "    for words in texts_tokens:\n",
    "        counter.update(words)\n",
    "    word2index = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for idx, word_count in enumerate(counter.most_common(SD.max_words)):\n",
    "        if word_count[1] >= SD.min_count:  \n",
    "            word2index[word_count[0]] = idx + 2  \n",
    "    n_words = len(word2index)\n",
    "    print('n_words:', n_words)\n",
    "\n",
    "    # data encode\n",
    "    print('data encoding')\n",
    "    seqs = []\n",
    "    for words in texts_tokens:\n",
    "        seqs.append([word2index.get(word, 1) for word in words])\n",
    "    seqs_padded = pad_sequences(seqs, maxlen=maxlen, padding='post', truncating='post')\n",
    "    s_labels = np.asarray(s_labels, dtype=int)\n",
    "    d_labels = np.asarray(d_labels, dtype=int)\n",
    "\n",
    "    # domain & train/val/test split\n",
    "    print('labeled data: domain & train/val/test splitting')\n",
    "    X_train, ys_train, yd_train = [], [], []\n",
    "    X_val, ys_val, yd_val = [], [], []\n",
    "    X_test_byd, ys_test_byd, yd_test_byd = {}, {}, {}\n",
    "    for d_id, d_name in enumerate(DOMAINS):\n",
    "        print(d_name, 'splitting')\n",
    "        seqs_padded_ofd = seqs_padded[(d_labels == d_id) & (s_labels != -1)]\n",
    "        slabels_ofd = s_labels[(d_labels == d_id) & (s_labels != -1)]\n",
    "        print(' * all:', seqs_padded_ofd.shape, slabels_ofd.shape)\n",
    "        (X_train_ofd, X_val_ofd, X_test_ofd), (y_train_ofd, y_val_ofd, y_test_ofd) = _tvt_split(seqs_padded_ofd, slabels_ofd)\n",
    "        # train data (add this domain)\n",
    "        X_train.extend(X_train_ofd)\n",
    "        ys_train.extend(y_train_ofd)\n",
    "        yd_train.extend([d_id] * len(X_train_ofd))\n",
    "        # val data\n",
    "        X_val.extend(X_val_ofd)\n",
    "        ys_val.extend(y_val_ofd)\n",
    "        yd_val.extend([d_id] * len(X_val_ofd))\n",
    "        # test data\n",
    "        X_test_byd[d_id] = X_test_ofd\n",
    "        ys_test_byd[d_id] = to_categorical(y_test_ofd, num_classes=2)\n",
    "        yd_test_byd[d_id] = to_categorical([d_id] * len(X_test_ofd), num_classes=len(DOMAINS))\n",
    "    X_train = np.asarray(X_train, dtype='int')\n",
    "    ys_train = to_categorical(ys_train, num_classes=2)\n",
    "    yd_train = to_categorical(yd_train, num_classes=len(DOMAINS))\n",
    "    X_val = np.asarray(X_val, dtype='int')\n",
    "    ys_val = to_categorical(ys_val, num_classes=2)\n",
    "    yd_val = to_categorical(yd_val, num_classes=len(DOMAINS))\n",
    "    \n",
    "    X_test = np.concatenate([X_test_byd[idx] for idx in range(len(DOMAINS))])\n",
    "    ys_test = np.concatenate([ys_test_byd[idx] for idx in range(len(DOMAINS))])\n",
    "    yd_test = np.concatenate([yd_test_byd[idx] for idx in range(len(DOMAINS))])\n",
    "\n",
    "    indices = list(range(len(X_train)))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X_train[indices]\n",
    "    ys_train = ys_train[indices]\n",
    "    yd_train = yd_train[indices]\n",
    "    print('combined labeled data:')\n",
    "    print('  - train:', X_train.shape, ys_train.shape, yd_train.shape)\n",
    "    print('  - val:', X_val.shape, ys_val.shape, yd_val.shape)\n",
    "    print('  - test:', X_test.shape, ys_test.shape, yd_test.shape)\n",
    "    for d_id, d_name in enumerate(DOMAINS):\n",
    "        print('  - test for {}:'.format(d_name[:3]), X_test_byd[d_id].shape, ys_test_byd[d_id].shape, yd_test_byd[d_id].shape)\n",
    "\n",
    "    print('loading word embeddings from glove')\n",
    "    embeddings = load_glove(embedding_dim=SD.embed_dim, desired=word2index.keys(), corpus_size=SD.glove_corpus)\n",
    "    print('processing embedding matrix')\n",
    "    embedding_mat = get_embedding_mat(embeddings, word2index, SD.embed_dim, idx_from=2)\n",
    "    SD.wv_weights = [embedding_mat]\n",
    "\n",
    "    SD.maxlen = maxlen\n",
    "    SD.n_words = n_words\n",
    "    SD.word2index = word2index\n",
    "    SD.X_train, SD.ys_train, SD.yd_train = X_train, ys_train, yd_train\n",
    "    SD.X_val, SD.ys_val, SD.yd_val = X_val, ys_val, yd_val\n",
    "    SD.X_test, SD.ys_test, SD.yd_test = X_test, ys_test, yd_test\n",
    "    SD.X_test_byd, SD.ys_test_byd, SD.yd_test_byd = X_test_byd, ys_test_byd, yd_test_byd\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    global SD\n",
    "\n",
    "    weights = SD.wv_weights\n",
    "\n",
    "    # the model\n",
    "    print('\\nbuilding the model')\n",
    "    inputs = layers.Input(shape=(SD.maxlen,))\n",
    "    embeddings = layers.Embedding(\n",
    "        input_dim=SD.n_words,\n",
    "        output_dim=SD.embed_dim,\n",
    "        input_length=SD.maxlen,\n",
    "        weights=weights)(inputs)\n",
    "    embeddings = layers.SpatialDropout1D(rate=SD.embed_dropout)(embeddings)\n",
    "\n",
    "    # domain part\n",
    "    d_repr = layers.Bidirectional(SD.RNN(\n",
    "        units=SD.rnn_dim,\n",
    "        return_sequences=False))(embeddings)\n",
    "    d_repr = layers.Dense(SD.hidden_dim, activation=SD.activation)(d_repr)\n",
    "    d_repr = layers.Dropout(SD.fc_dropout)(d_repr)\n",
    "    d_pred = layers.Dense(len(DOMAINS), activation='softmax', name='d_pred')(d_repr)\n",
    "\n",
    "    # sentiment part\n",
    "    # use domain representation as attention\n",
    "    episodes = layers.Bidirectional(SD.RNN(\n",
    "        units=SD.rnn_dim,\n",
    "        return_sequences=True))(embeddings)\n",
    "    selected, _ = att_process(candidates=episodes, att=d_repr)\n",
    "    s_repr = layers.Dense(SD.hidden_dim, activation=SD.activation)(selected)\n",
    "    s_repr = layers.Dropout(SD.fc_dropout)(s_repr)\n",
    "    s_pred = layers.Dense(2, activation='softmax', name='s_pred')(s_repr)\n",
    "\n",
    "    # model\n",
    "    model = Model(\n",
    "        inputs=inputs,\n",
    "        outputs=[s_pred, d_pred])\n",
    "    model.compile(optimizer=SD.optimizer, metrics=['acc'], loss={\n",
    "        's_pred': 'categorical_crossentropy',\n",
    "        'd_pred': 'categorical_crossentropy'\n",
    "    }, loss_weights={\n",
    "        's_pred': 1,\n",
    "        'd_pred': SD.coef\n",
    "    })\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_test(model):\n",
    "    global SD\n",
    "\n",
    "    # training\n",
    "    updater = UpdateMonitor()\n",
    "    reducer = callbacks.ReduceLROnPlateau(factor=SD.lr_factor, patience=SD.lr_patience, verbose=1)\n",
    "    stopper = callbacks.EarlyStopping(patience=SD.stop_patience, verbose=1)\n",
    "    cbks = [updater, reducer, stopper]\n",
    "    print('\\ntraining model')\n",
    "    model.fit(\n",
    "        SD.X_train,\n",
    "        [SD.ys_train, SD.yd_train],\n",
    "        validation_data=(SD.X_val, [SD.ys_val, SD.yd_val]),\n",
    "        shuffle=True, batch_size=SD.batch_size, epochs=SD.epochs, verbose=2,\n",
    "        callbacks=cbks)\n",
    "\n",
    "    # evaluation\n",
    "    print('\\nTest evaluation:')\n",
    "    for d_id, d_name in enumerate(DOMAINS):\n",
    "        scores = model.evaluate(\n",
    "            SD.X_test_byd[d_id],\n",
    "            [SD.ys_test_byd[d_id], SD.yd_test_byd[d_id]],\n",
    "            batch_size=SD.batch_size, verbose=0)\n",
    "        print('{} acc: {:.4f}'.format(d_name[:3], scores[-2]))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    make_data()\n",
    "\n",
    "    # build & compile model\n",
    "    model = get_model()\n",
    "\n",
    "    # train and test\n",
    "    train_and_test(model)\n",
    "\n",
    "    print('\\nprocess finished ~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0a516ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test evaluation:\n",
      "boo acc: 0.5099\n",
      "dvd acc: 0.5099\n",
      "ele acc: 0.4950\n",
      "kit acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "    print('\\nTest evaluation:')\n",
    "    for d_id, d_name in enumerate(DOMAINS):\n",
    "        scores = model.evaluate(\n",
    "            SD.X_test_byd[d_id],\n",
    "            [SD.ys_test_byd[d_id], SD.yd_test_byd[d_id]],\n",
    "            batch_size=SD.batch_size, verbose=0)\n",
    "        print('{} acc: {:.4f}'.format(d_name[:3], scores[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7674f3",
   "metadata": {},
   "source": [
    "## This model is run only for 20 epochs to show the working on my local system so has lower metric/accuracy values (and very significant training time), I trained this on powerful Titan X GPU for 5 experiments (architectures) and have their metrics below(thanks to my friend Anup for the access to the GPU!)\n",
    "### Note: The log files for all 5 experiments which has the entire run is included in the log folder, those log files have the metric values shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098d50e",
   "metadata": {},
   "source": [
    "## The metrics comparing all the experiments done are shown below after running all experiments and making graphs from them, the pdf report also contains Network Architectures for each experiment using Tensorboard.\n",
    "\n",
    "#### The Experiments :\n",
    "\n",
    "**EXPERIMENT No.**|**AIM**\n",
    ":-----:|:-----:\n",
    "1|Train the state of art model proposed by Yuan et al. and obtain the metrics for comparison.\n",
    "2|Add a dense layer before the final layer of the sentiment module. The number of neurons is kept equal to the number of domains.\n",
    "3|Use a combination of LSTM and GRU in the hidden layers. Use LSTM layer as the bidirectional RNN in the domain module and GRU in the sentiment module.\n",
    "4|Use a combination of LSTM and GRU in the hidden layers. Use GRU layer as the bidirectional RNN in the domain module and LSTM in the sentiment module.\n",
    "5|Use GRU layer as the bidirectional RNN in the domain module as well as in the sentiment module.\n",
    "\n",
    "#### The Metrics:\n",
    "\n",
    "**Metric**|**Definition**\n",
    ":-----:|:-----:\n",
    "d\\_pred\\_acc | The accuracy of  the model to predict the domain. Accuracy is the fraction of predictions our model got right.\n",
    "d\\_pred\\_loss| The loss in the model to predict the domain. Loss is a number indicating how bad the model's prediction was on a single example. \n",
    "loss|The complete loss from both the domains. We use mean squared error for finding the loss in our models.\n",
    "s\\_pred\\_acc | The accuracy of  the model to predict the sentiment. \n",
    "s\\_pred\\_loss| The loss in the model to predict the sentiment. \n",
    "val\\_d\\_pred\\_acc | The accuracy of  the model to predict the domain during validation. Accuracy is the fraction of predictions our model got right.\n",
    "val\\_d\\_pred\\_loss| The loss in the model to predict the domain during validation. Loss is a number indicating how bad the model's prediction was on a single example. \n",
    "val\\_loss|The complete loss from both the domains during validation. We use mean squared error for finding the loss in our models.\n",
    "val\\_s\\_pred\\_acc | The accuracy of  the model to predict the sentiment during validation. \n",
    "val\\_s\\_pred\\_loss| The loss in the model to predict the sentiment during validation. \n",
    "\n",
    "### Note: These Images are added kept in the same directory as the Notebook for these to be displayed properly by Markdown.\n",
    "\n",
    "\n",
    "![alt text](image5.png \"Title\")\n",
    "![alt text](image6.png \"Title\")\n",
    "![alt text](image13.png \"Title\")\n",
    "![alt text](image15.png \"Title\")\n",
    "![alt text](image16.png \"Title\")\n",
    "![alt text](image14.png \"Title\")\n",
    "![alt text](image10.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b407e50",
   "metadata": {},
   "source": [
    "## The comparison in training time of different Experiments/Architectures\n",
    "![alt text](image3.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe36bb6",
   "metadata": {},
   "source": [
    "## The final validation accuracy comparison\n",
    "![alt text](image4.png \"Title\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
